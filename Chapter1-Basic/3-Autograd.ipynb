{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oIfkscXCrgyv"
   },
   "source": [
    "# Autograd    \n",
    "`torch.autograd` 就是为方便用户使用，而专门开发的一套自动求导引擎，它能够根据输入和前向传播过程自动构建计算图，并执行反向传播。\n",
    "\n",
    "计算图(Computation Graph)是现代深度学习框架如PyTorch和TensorFlow等的核心，其为高效自动求导算法——反向传播(Back Propogation)提供了理论支持，了解计算图在实际写程序过程中会有极大的帮助。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-3eEmVIcLJrs"
   },
   "source": [
    "## 3.1 Autograd的前世今生       \n",
    "深度学习的算法本质上是通过反向传播求导数，而PyTorch的**`autograd`**模块则实现了此功能。在Tensor上的所有操作，autograd都能为它们自动提供微分，避免了手动计算导数的复杂过程。\n",
    " \n",
    "~~`autograd.Variable`是Autograd中的核心类，它简单封装了Tensor，并支持几乎所有Tensor有的操作。Tensor在被封装为Variable之后，可以调用它的`.backward`实现反向传播，自动计算所有梯度~~\n",
    "\n",
    "\n",
    "~~Variable主要包含三个属性~~      \n",
    "~~- `data`：保存Variable所包含的Tensor~~       \n",
    "~~- `grad`：保存`data`对应的梯度，`grad`也是个Variable，而不是Tensor，它和`data`的形状一样~~        \n",
    "~~- `grad_fn`：指向一个`Function`对象，这个`Function`用来反向传播计算输入的梯度~~             \n",
    "\n",
    "---     \n",
    "\n",
    "\n",
    "  *从0.4起, Variable 正式合并入Tensor, Variable 本来实现的自动微分功能，Tensor就能支持。读者还是可以使用Variable(tensor), 但是这个操作其实什么都没做。建议读者以后直接使用tensor*. \n",
    "  \n",
    "  要想使得Tensor使用autograd功能，只需要设置**`tensor.requries_grad=True`**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HSotMu90-NqB"
   },
   "source": [
    "## 3.2  requires_grad\n",
    "PyTorch在autograd模块中实现了计算图的相关功能，autograd中的核心数据结构是Variable。从v0.4版本起，Variable和Tensor合并。我们可以认为需要求导(requires_grad)的tensor即Variable. autograd记录对tensor的操作记录用来构建计算图。\n",
    "\n",
    "Variable提供了大部分tensor支持的函数，但其不支持部分`inplace`函数，因这些函数会修改tensor自身，而在反向传播中，variable需要缓存原来的tensor来计算反向传播梯度。如果想要计算各个Variable的梯度，只需调用根节点variable的`backward`方法，autograd会自动沿着计算图反向传播，计算每一个叶子节点的梯度。\n",
    "\n",
    "***`variable.backward(gradient=None, retain_graph=None, create_graph=None)`***        \n",
    "主要有如下参数：\n",
    "\n",
    "- grad_variables：形状与variable一致，对于`y.backward()`，grad_variables相当于链式法则${dz \\over dx}={dz \\over dy} \\times {dy \\over dx}$中的$\\textbf {dz} \\over \\textbf {dy}$。grad_variables也可以是tensor或序列。\n",
    "- retain_graph：反向传播需要缓存一些中间结果，反向传播之后，这些缓存就被清空，可通过指定这个参数不清空缓存，用来多次反向传播。\n",
    "- create_graph：对反向传播过程再次构建计算图，可通过`backward of backward`实现求高阶导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ODtAByD8TCi"
   },
   "outputs": [],
   "source": [
    "import torch as t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2505,
     "status": "ok",
     "timestamp": 1557399783388,
     "user": {
      "displayName": "wes V",
      "photoUrl": "https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg",
      "userId": "14257616964780137484"
     },
     "user_tz": -480
    },
    "id": "yPZ6_o6W9OSv",
    "outputId": "9953868e-0cfd-43d7-c2b5-a1b553a374f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8913, 0.5391, 0.6613],\n",
       "        [0.6033, 0.9297, 0.0086]], requires_grad=True)"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.randn(2, 3, requires_grad=True)\n",
    "\n",
    "a = t.randn(2, 3).requires_grad_()\n",
    "\n",
    "a = t.rand(2, 3)\n",
    "a.requires_grad=True\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2XxnMNeO9sCb"
   },
   "outputs": [],
   "source": [
    "b = t.zeros(2, 3, requires_grad=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2485,
     "status": "ok",
     "timestamp": 1557399783392,
     "user": {
      "displayName": "wes V",
      "photoUrl": "https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg",
      "userId": "14257616964780137484"
     },
     "user_tz": -480
    },
    "id": "5tBXrLjy_NOJ",
    "outputId": "a72dc413-ab1e-4e54-fe62-02cf174079dc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AddBackward0 at 0x7f613807e518>"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a.add(b)\n",
    "c.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2459,
     "status": "ok",
     "timestamp": 1557399783393,
     "user": {
      "displayName": "wes V",
      "photoUrl": "https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg",
      "userId": "14257616964780137484"
     },
     "user_tz": -480
    },
    "id": "VTWfwi9j_law",
    "outputId": "119ac533-a09b-4627-f743-415c7f749c63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.6332, grad_fn=<SumBackward0>)\n",
      "tensor(3.6332, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "d = c.sum()\n",
    "print(d)\n",
    "d.backward\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2446,
     "status": "ok",
     "timestamp": 1557399783396,
     "user": {
      "displayName": "wes V",
      "photoUrl": "https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg",
      "userId": "14257616964780137484"
     },
     "user_tz": -480
    },
    "id": "31DhpXn4_ssi",
    "outputId": "3adbe5e1-d689-4c24-bf8a-5beea3243cf0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, False, False)"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.is_leaf, b.is_leaf, c.is_leaf, d.is_leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OV04_X3JAWGp"
   },
   "source": [
    "利用下面的函数，比对一下自动求导与手动求导：      \n",
    "$$\n",
    "y = x^2\\bullet e^x\n",
    "$$\n",
    "它的导函数是：\n",
    "$$\n",
    "{dy \\over dx} = 2x\\bullet e^x + x^2 \\bullet e^x\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zyu4MrhBAtyM"
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    y = x** 2 * t.exp(x)\n",
    "    return y\n",
    "\n",
    "def gradf(x):\n",
    "    dx = 2*x*t.exp(x) + x**2*t.exp(x)\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2432,
     "status": "ok",
     "timestamp": 1557399783400,
     "user": {
      "displayName": "wes V",
      "photoUrl": "https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg",
      "userId": "14257616964780137484"
     },
     "user_tz": -480
    },
    "id": "1tNa1Gw3BM3w",
    "outputId": "c25cad94-6ba6-4aed-923c-35399a7fb2ca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.0799e-02, 2.6712e-01, 2.8743e+01, 1.2769e-03],\n",
       "        [8.3794e+00, 4.0024e-02, 2.3730e-01, 2.4196e-02],\n",
       "        [2.9234e+00, 2.7208e-04, 3.1842e+01, 3.0278e+00]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.randn(3, 4, requires_grad=True)\n",
    "y = f(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2415,
     "status": "ok",
     "timestamp": 1557399783401,
     "user": {
      "displayName": "wes V",
      "photoUrl": "https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg",
      "userId": "14257616964780137484"
     },
     "user_tz": -480
    },
    "id": "N2YmZ_CtBtLT",
    "outputId": "9fe1364b-a948-4600-8fd6-273fce8e1497"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.2935e-01, -4.4218e-01,  5.7688e+01, -6.8901e-02],\n",
       "        [ 2.0166e+01,  4.7840e-01,  1.4267e+00,  3.5864e-01],\n",
       "        [ 8.6308e+00,  3.3533e-02,  6.3099e+01,  8.8707e+00]])"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.backward(t.ones(y.size())) # gradient形状与y一致\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2398,
     "status": "ok",
     "timestamp": 1557399783402,
     "user": {
      "displayName": "wes V",
      "photoUrl": "https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg",
      "userId": "14257616964780137484"
     },
     "user_tz": -480
    },
    "id": "xhNcjr-fB9uU",
    "outputId": "b3f000b5-465c-4c10-8627-9e391915ac84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.2935e-01, -4.4218e-01,  5.7688e+01, -6.8901e-02],\n",
       "        [ 2.0166e+01,  4.7840e-01,  1.4267e+00,  3.5864e-01],\n",
       "        [ 8.6308e+00,  3.3533e-02,  6.3099e+01,  8.8707e+00]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 手动计算  \n",
    "gradf(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GJXzbI7DCX3U"
   },
   "source": [
    "## 3.3 计算图     \n",
    "PyTorch中`autograd`的底层采用了计算图，计算图是一种特殊的有向无环图（DAG），用于记录算子与变量之间的关系。一般用矩形表示算子，椭圆形表示变量。       \n",
    "表达式 $ \\textbf {z = wx + b}$可分解为 $\\textbf{y = wx}$和 $\\textbf{z = y + b}$，其计算图如图所示，图中`MUL`，`ADD`都是算子，$\\textbf{w}$，$\\textbf{x}$，$\\textbf{b}$即变量。\n",
    "\n",
    "![computation graph](img/com_graph.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2FVUGB9QFQAx"
   },
   "source": [
    "如上有向无环图中，$\\textbf{X}$和$\\textbf{b}$是叶子节点（leaf node），这些节点通常由用户自己创建，不依赖于其他变量。$\\textbf{z}$称为根节点，是计算图的最终目标。利用链式法则很容易求得各个叶子节点的梯度。\n",
    "$${\\partial z \\over \\partial b} = 1,\\space {\\partial z \\over \\partial y} = 1\\\\\n",
    "{\\partial y \\over \\partial w }= x,{\\partial y \\over \\partial x}= w\\\\\n",
    "{\\partial z \\over \\partial x}= {\\partial z \\over \\partial y} {\\partial y \\over \\partial x}=1 * w\\\\\n",
    "{\\partial z \\over \\partial w}= {\\partial z \\over \\partial y} {\\partial y \\over \\partial w}=1 * x\\\\\n",
    "$$\n",
    "而有了计算图，上述链式求导即可利用计算图的反向传播自动完成，如图所示。\n",
    "\n",
    "![反向传播](img/com_graph_backward.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jN2J7LJ2FjOh"
   },
   "source": [
    "在PyTorch实现中，autograd会随着用户的操作，记录生成当前variable的所有操作，并由此建立一个有向无环图。用户每进行一个操作，相应的计算图就会发生改变。更底层的实现中，图中记录了操作`Function`，每一个变量在图中的位置可通过其`grad_fn`属性在图中的位置推测得到。      \n",
    "  \n",
    "在反向传播过程中，autograd沿着这个图从当前变量（根节点$\\textbf{z}$）溯源，可以利用链式求导法则计算所有叶子节点的梯度。每一个前向传播操作的函数都有与之对应的反向传播函数用来计算输入的各个variable的梯度，这些函数的函数名通常以`Backward`结尾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z6CiwPZTGrQQ"
   },
   "outputs": [],
   "source": [
    "x = t.ones(1)\n",
    "w = t.rand(1, requires_grad=True)\n",
    "b = t.rand(1, requires_grad=True)\n",
    "\n",
    "y = w * x\n",
    "z = y + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2389,
     "status": "ok",
     "timestamp": 1557399783406,
     "user": {
      "displayName": "wes V",
      "photoUrl": "https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg",
      "userId": "14257616964780137484"
     },
     "user_tz": -480
    },
    "id": "kAfw-bI8HNGc",
    "outputId": "902f72a5-d309-4de9-e4d2-9e9e52ff0bc8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True, True, True, True)"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y依赖于需要求导的w故为True\n",
    "x.requires_grad, w.requires_grad, b.requires_grad, y.requires_grad, z.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2365,
     "status": "ok",
     "timestamp": 1557399783407,
     "user": {
      "displayName": "wes V",
      "photoUrl": "https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg",
      "userId": "14257616964780137484"
     },
     "user_tz": -480
    },
    "id": "FiVjN3f4HW5h",
    "outputId": "0f27c8e2-a25b-4996-d367-c833126b2479"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True, False, False)"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.is_leaf, w.is_leaf, b.is_leaf, y.is_leaf, z.is_leaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2960,
     "status": "ok",
     "timestamp": 1557399784012,
     "user": {
      "displayName": "wes V",
      "photoUrl": "https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg",
      "userId": "14257616964780137484"
     },
     "user_tz": -480
    },
    "id": "SoOCQ1qaIBIu",
    "outputId": "1f70d578-1c3e-4820-e246-19dc08ffa33c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AddBackward0 at 0x7f60ec54bef0>"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grad_fn：查看variable的反向传播函数\n",
    "z.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2952,
     "status": "ok",
     "timestamp": 1557399784014,
     "user": {
      "displayName": "wes V",
      "photoUrl": "https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg",
      "userId": "14257616964780137484"
     },
     "user_tz": -480
    },
    "id": "754vumJzILMl",
    "outputId": "4de12831-d8eb-441c-f062-89bdc3771c3d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<MulBackward0 at 0x7f60ec56d7b8>, 0),\n",
       " (<AccumulateGrad at 0x7f60ec56d240>, 0))"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# next_functions：保存grad_fn的输入，是一个tuple，其中的元素也是Function\n",
    "# 第一个是y，它是乘法(mul)的输出，所以y.grad_fn是MulBackward\n",
    "# 第二个是b，它是叶子节点，由用户创建，grad_fn为None，但是有\n",
    "z.grad_fn.next_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2943,
     "status": "ok",
     "timestamp": 1557399784015,
     "user": {
      "displayName": "wes V",
      "photoUrl": "https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg",
      "userId": "14257616964780137484"
     },
     "user_tz": -480
    },
    "id": "XX3Hqh2RIRND",
    "outputId": "dbc7a259-d447-48d3-ed23-3c3ecf1c447c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.grad_fn.next_functions[0][0] == y.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2934,
     "status": "ok",
     "timestamp": 1557399784016,
     "user": {
      "displayName": "wes V",
      "photoUrl": "https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg",
      "userId": "14257616964780137484"
     },
     "user_tz": -480
    },
    "id": "1dpwvXI2I_3-",
    "outputId": "4e2155ba-9e8b-4917-a1bb-1b50009526fd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<AccumulateGrad at 0x7f60ec56d0b8>, 0), (None, 0))"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# w, b\n",
    "y.grad_fn.next_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2925,
     "status": "ok",
     "timestamp": 1557399784017,
     "user": {
      "displayName": "wes V",
      "photoUrl": "https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg",
      "userId": "14257616964780137484"
     },
     "user_tz": -480
    },
    "id": "cWERdScuJHYN",
    "outputId": "d1c48488-e540-4a2e-c54b-9931ecf0be43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.grad_fn, x.grad_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2916,
     "status": "ok",
     "timestamp": 1557399784018,
     "user": {
      "displayName": "wes V",
      "photoUrl": "https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg",
      "userId": "14257616964780137484"
     },
     "user_tz": -480
    },
    "id": "fhE0jHoaJT5E",
    "outputId": "85ba3757-ef40-4885-9cdd-cc6054d1754c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.backward(retain_graph=True)\n",
    "w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2906,
     "status": "ok",
     "timestamp": 1557399784019,
     "user": {
      "displayName": "wes V",
      "photoUrl": "https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg",
      "userId": "14257616964780137484"
     },
     "user_tz": -480
    },
    "id": "yQtH4HP7WMw2",
    "outputId": "62995b21-276f-41e1-8048-1b14de22088c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.])"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.backward()\n",
    "w.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "py8CgD1rWPtP"
   },
   "source": [
    "PyTorch使用的是动态图，它的计算图在每次前向传播时都是从头开始构建，所以它能够使用Python控制语句（如for、if等）根据需求创建计算图。      \n",
    "   \n",
    "这点在自然语言处理领域中很有用，它意味着你不需要事先构建所有可能用到的图的路径，图在运行时才构建。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ApgeqYzXCMF"
   },
   "source": [
    "变量的`requires_grad`属性默认为False，如果某一个节点requires_grad被设置为True，那么所有依赖它的节点`requires_grad`都是True。这其实很好理解，对于$ \\textbf{x}\\to \\textbf{y} \\to \\textbf{z}$，x.requires_grad = True，当需要计算$\\partial z \\over \\partial x$时，根据链式法则，$\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} \\frac{\\partial y}{\\partial x}$，自然也需要求$ \\frac{\\partial z}{\\partial y}$，所以`y.requires_grad` 会被自动标为True. \n",
    "\n",
    "\n",
    "\n",
    "有些时候我们可能不希望autograd对tensor求导。认为求导需要缓存许多中间结构，增加额外的内存/显存开销，那么我们可以关闭自动求导。对于不需要反向传播的情景（如inference，即测试推理时），关闭自动求导可实现一定程度的速度提升，并节省约一半显存，因其不需要分配空间计算梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2898,
     "status": "ok",
     "timestamp": 1557399784020,
     "user": {
      "displayName": "wes V",
      "photoUrl": "https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg",
      "userId": "14257616964780137484"
     },
     "user_tz": -480
    },
    "id": "yJN5PwqzXjSo",
    "outputId": "447d9581-2abf-4f1a-d5ea-6555fd09fb55"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.4142], requires_grad=True),\n",
       " tensor([1.], requires_grad=True),\n",
       " tensor([0.4142], grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 21,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = t.rand(1, requires_grad=True)\n",
    "x = t.ones(1, requires_grad=True)\n",
    "y = w * x\n",
    "w, x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OQ29gtkNa_e_"
   },
   "source": [
    "### 设置不自动求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2884,
     "status": "ok",
     "timestamp": 1557399784021,
     "user": {
      "displayName": "wes V",
      "photoUrl": "https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg",
      "userId": "14257616964780137484"
     },
     "user_tz": -480
    },
    "id": "b5_8n80GYd2W",
    "outputId": "dbbf4e2a-6178-4c8a-f8f2-bbd23ce3921a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1583], requires_grad=True), tensor([1.]), tensor([0.1583]))"
      ]
     },
     "execution_count": 22,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with t.no_grad():\n",
    "    w = t.rand(1, requires_grad=True)\n",
    "    x = t.ones(1)\n",
    "    y = w * x\n",
    "# y依赖于w和x，但x和y\n",
    "w, x, y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2869,
     "status": "ok",
     "timestamp": 1557399784023,
     "user": {
      "displayName": "wes V",
      "photoUrl": "https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg",
      "userId": "14257616964780137484"
     },
     "user_tz": -480
    },
    "id": "9NorZ-yLZk-U",
    "outputId": "b920a985-7472-46e3-cd21-22e69803bda9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.0417], requires_grad=True), tensor([1.]), tensor([0.0417]))"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.set_grad_enabled(False) \n",
    "w = t.rand(1, requires_grad=True)\n",
    "x = t.ones(1)\n",
    "y = w * x\n",
    "# y依赖于w和x，但x和y\n",
    "w, x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2857,
     "status": "ok",
     "timestamp": 1557399784025,
     "user": {
      "displayName": "wes V",
      "photoUrl": "https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg",
      "userId": "14257616964780137484"
     },
     "user_tz": -480
    },
    "id": "JuLYHT3YZ04z",
    "outputId": "8abda27e-5c5c-4a59-8ca4-108d3d863977"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x7f60ec56d860>"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.set_grad_enabled(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hY4eqzwLbrKp"
   },
   "source": [
    "### 修改Tensor数值    \n",
    "如果我们希望对tensor，但是又不希望被记录, 可以使用 **`tensor.data` 或 `tensor.detach()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2842,
     "status": "ok",
     "timestamp": 1557399784026,
     "user": {
      "displayName": "wes V",
      "photoUrl": "https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg",
      "userId": "14257616964780137484"
     },
     "user_tz": -480
    },
    "id": "ops7dKsGj0Lw",
    "outputId": "3486ee31-32e5-4f3e-a769-a3af7b3355a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.ones(3,4,requires_grad=True)\n",
    "b = t.ones(3,4,requires_grad=True)\n",
    "c = a * b  \n",
    "a.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2827,
     "status": "ok",
     "timestamp": 1557399784027,
     "user": {
      "displayName": "wes V",
      "photoUrl": "https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg",
      "userId": "14257616964780137484"
     },
     "user_tz": -480
    },
    "id": "vdQEdH54kLwv",
    "outputId": "34d20cb1-ca95-4bed-d524-ad02d7c28964"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.data.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2826,
     "status": "ok",
     "timestamp": 1557399784036,
     "user": {
      "displayName": "wes V",
      "photoUrl": "https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg",
      "userId": "14257616964780137484"
     },
     "user_tz": -480
    },
    "id": "zOC1q4fQkZuu",
    "outputId": "3193fe56-385f-44f2-e634-006977af49e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = a.data.sigmoid_()\n",
    "d.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2772,
     "status": "ok",
     "timestamp": 1557399784038,
     "user": {
      "displayName": "wes V",
      "photoUrl": "https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg",
      "userId": "14257616964780137484"
     },
     "user_tz": -480
    },
    "id": "P5QsfMCAkhY0",
    "outputId": "65f99b71-a38f-4d29-e410-b5b86e3340e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7311, 0.7311, 0.7311, 0.7311],\n",
       "        [0.7311, 0.7311, 0.7311, 0.7311],\n",
       "        [0.7311, 0.7311, 0.7311, 0.7311]], requires_grad=True)"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2762,
     "status": "ok",
     "timestamp": 1557399784041,
     "user": {
      "displayName": "wes V",
      "photoUrl": "https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg",
      "userId": "14257616964780137484"
     },
     "user_tz": -480
    },
    "id": "dDuK4mBRklni",
    "outputId": "56397d56-d18c-40ba-99bf-b6d540db100e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.7311, 0.7311, 0.7311, 0.7311],\n",
       "         [0.7311, 0.7311, 0.7311, 0.7311],\n",
       "         [0.7311, 0.7311, 0.7311, 0.7311]]), False)"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor.datch()\n",
    "# 近似于 tensor=a.data, 但是如果tensor被修改，backward可能会报错\n",
    "tensor = a.detach()\n",
    "tensor, tensor.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2742,
     "status": "ok",
     "timestamp": 1557399784042,
     "user": {
      "displayName": "wes V",
      "photoUrl": "https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg",
      "userId": "14257616964780137484"
     },
     "user_tz": -480
    },
    "id": "gnyf95Kjlsec",
    "outputId": "2e915db2-98f5-4cc4-91f3-f8b464c30b33"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.7311), tensor(0.))"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 统计tensor的一些指标，并不希望被记录\n",
    "mean = tensor.mean()\n",
    "std = tensor.std()\n",
    "mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CATv1Iexmih1"
   },
   "source": [
    "### 查看变量梯度    \n",
    "在反向传播过程中非叶子节点的导数计算完之后即被清空。若想查看这些变量的梯度，有两种方法：\n",
    "- 使用autograd.grad函数\n",
    "- 使用hook\n",
    "\n",
    "`autograd.grad`和`hook`方法都是很强大的工具。     \n",
    "推荐使用`hook`方法，但是在实际使用中应尽量避免修改grad的值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2722,
     "status": "ok",
     "timestamp": 1557399784044,
     "user": {
      "displayName": "wes V",
      "photoUrl": "https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg",
      "userId": "14257616964780137484"
     },
     "user_tz": -480
    },
    "id": "S500yFysxujz",
    "outputId": "d8ea2bca-0458-4347-d61d-0bcea517195f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5200], requires_grad=True),\n",
       " tensor([1.], requires_grad=True),\n",
       " tensor([0.5200], grad_fn=<MulBackward0>),\n",
       " tensor(0.5200, grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = t.rand(1, requires_grad=True)\n",
    "x = t.ones(1, requires_grad=True)\n",
    "y = w * x\n",
    "z = y.sum()\n",
    "w, x, y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2707,
     "status": "ok",
     "timestamp": 1557399784046,
     "user": {
      "displayName": "wes V",
      "photoUrl": "https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg",
      "userId": "14257616964780137484"
     },
     "user_tz": -480
    },
    "id": "j0GBgHKpx430",
    "outputId": "9b876318-b893-4e0c-8534-bdf8d7df5cbf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.]), tensor([0.5200]), None)"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 非叶子节点grad计算完之后自动清空，y.grad是None\n",
    "z.backward()\n",
    "w.grad, x.grad, y.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2686,
     "status": "ok",
     "timestamp": 1557399784050,
     "user": {
      "displayName": "wes V",
      "photoUrl": "https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg",
      "userId": "14257616964780137484"
     },
     "user_tz": -480
    },
    "id": "bP67mmDPyDQL",
    "outputId": "c09d1dc3-18ab-46d2-8867-c816cf9f907e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5702, 0.0767, 0.8931], requires_grad=True),\n",
       " tensor([1., 1., 1.], requires_grad=True),\n",
       " tensor([0.5702, 0.0767, 0.8931], grad_fn=<MulBackward0>),\n",
       " tensor(1.5401, grad_fn=<SumBackward0>))"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 方法一：\n",
    "w = t.rand(3, requires_grad=True)\n",
    "x = t.ones(3, requires_grad=True)\n",
    "y = w * x\n",
    "z = y.sum()\n",
    "w, x, y, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2670,
     "status": "ok",
     "timestamp": 1557399784053,
     "user": {
      "displayName": "wes V",
      "photoUrl": "https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg",
      "userId": "14257616964780137484"
     },
     "user_tz": -480
    },
    "id": "kBo8OOwvzoun",
    "outputId": "bafded55-26c8-444e-a044-5b86f57dd066"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1.]),)"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.autograd.grad(z, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2662,
     "status": "ok",
     "timestamp": 1557399784055,
     "user": {
      "displayName": "wes V",
      "photoUrl": "https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg",
      "userId": "14257616964780137484"
     },
     "user_tz": -480
    },
    "id": "EdKFPvZyz0rQ",
    "outputId": "621d9588-19e8-4196-8c0f-98e7e81a9f60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y的梯度： tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "# 方法二：使用hook\n",
    "# hook是一个函数，输入是梯度，不应有返回值\n",
    "def variable_hook(grad):\n",
    "    print('y的梯度：', grad)\n",
    "    \n",
    "w = t.rand(3, requires_grad=True)\n",
    "x = t.ones(3, requires_grad=True)\n",
    "y = w * x\n",
    "z = y.sum()\n",
    "\n",
    "hook = y.register_hook(variable_hook)\n",
    "z.backward()\n",
    "\n",
    "# 若不重复使用hook，则移除\n",
    "hook.remove()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "13Yh0dzN0y-K"
   },
   "source": [
    "关于 variable中grad属性和backward函数`grad_variables`参数的含义：\n",
    "\n",
    "- variable $\\textbf{x}$的梯度是目标函数${f(x)} $对$\\textbf{x}$的梯度，$\\frac{df(x)}{dx} = (\\frac {df(x)}{dx_0},\\frac {df(x)}{dx_1},...,\\frac {df(x)}{dx_N})$，形状和$\\textbf{x}$一致。\n",
    "- 对于y.backward(grad_variables)中的grad_variables相当于链式求导法则中的$\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} \\frac{\\partial y}{\\partial x}$中的$\\frac{\\partial z}{\\partial y}$。z是目标函数，一般是一个标量，故而$\\frac{\\partial z}{\\partial y}$的形状与variable $\\textbf{y}$的形状一致。`z.backward()`在一定程度上等价于y.backward(grad_y)。`z.backward()`省略了grad_variables参数，是因为$z$是一个标量，而$\\frac{\\partial z}{\\partial z} = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2655,
     "status": "ok",
     "timestamp": 1557399784057,
     "user": {
      "displayName": "wes V",
      "photoUrl": "https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg",
      "userId": "14257616964780137484"
     },
     "user_tz": -480
    },
    "id": "fy16ZsJd10Du",
    "outputId": "6098e35f-b273-4c67-bc3c-879b0c7141c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2.], requires_grad=True) tensor([0., 3., 8.], grad_fn=<AddBackward0>) tensor(11., grad_fn=<SumBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 6.])"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.Tensor([0,1,2]).requires_grad_()\n",
    "y = x**2 + 2*x\n",
    "z = y.sum()\n",
    "print(x, y, z)\n",
    "\n",
    "z.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2648,
     "status": "ok",
     "timestamp": 1557399784059,
     "user": {
      "displayName": "wes V",
      "photoUrl": "https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg",
      "userId": "14257616964780137484"
     },
     "user_tz": -480
    },
    "id": "VJ7H4G-b5XHW",
    "outputId": "bf6a0d15-97bc-48e9-9bd3-822bef7c3f3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 4., 6.])"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.Tensor([0,1,2]).requires_grad_()\n",
    "y = x**2 + 2*x\n",
    "z = y.sum()\n",
    "\n",
    "y_gredient = t.Tensor([1, 1, 1]) # dz/dy\n",
    "y.backward(y_gredient)\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ILZoPhf5MDdi"
   },
   "source": [
    "另外值得注意的是，只有对variable的操作才能使用autograd，如果对variable的data直接进行操作，将无法使用反向传播。除了对参数初始化，一般我们不会修改variable.data的值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AqtRW2RLMTcm"
   },
   "source": [
    "### 计算图的特点：\n",
    "- autograd根据用户对variable的操作构建其计算图。对变量的操作抽象为`Function`。\n",
    "- 对于那些不是任何函数(Function)的输出，由用户创建的节点称为叶子节点，叶子节点的`grad_fn`为None。叶子节点中需要求导的variable，具有`AccumulateGrad`标识，因其梯度是累加的。\n",
    "- variable默认是不需要求导的，即`requires_grad`属性默认为False，如果某一个节点requires_grad被设置为True，那么所有依赖它的节点`requires_grad`都为True。\n",
    "- variable的`volatile`属性默认为False，如果某一个variable的`volatile`属性被设为True，那么所有依赖它的节点`volatile`属性都为True。volatile属性为True的节点不会求导，volatile的优先级比`requires_grad`高。\n",
    "- 多次反向传播时，梯度是累加的。反向传播的中间缓存会被清空，为进行多次反向传播需指定`retain_graph`=True来保存这些缓存。\n",
    "- 非叶子节点的梯度计算完之后即被清空，可以使用`autograd.grad`或`hook`技术获取非叶子节点的值。\n",
    "- variable的grad与data形状一致，应避免直接修改variable.data，因为对data的直接操作无法利用autograd进行反向传播\n",
    "- 反向传播函数`backward`的参数`grad_variables`可以看成链式求导的中间结果，如果是标量，可以省略，默认为1\n",
    "- PyTorch采用动态图设计，可以很方便地查看中间层的输出，动态的设计计算图结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "43Qf2VcrJ2QL"
   },
   "source": [
    "## 3.4 Autograd拓展         \n",
    "\n",
    "目前绝大多数函数都可以使用`autograd`实现反向求导，但如果需要自己写一个复杂的函数，不支持自动反向求导怎么办? 写一个`Function`，实现它的前向传播和反向传播代码，`Function`对应于计算图中的矩形， 它接收参数，计算并返回结果。     \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "- 自定义的Function需要继承autograd.Function，没有构造函数`__init__`，forward和backward函数都是静态方法\n",
    "- backward函数的输出和forward函数的输入一一对应，backward函数的输入和forward函数的输出一一对应\n",
    "- backward函数的grad_output参数即t.autograd.backward中的`grad_variables`\n",
    "- 如果某一个输入不需要求导，直接返回None，如forward中的输入参数x_requires_grad显然无法对它求导，直接返回None即可\n",
    "- 反向传播可能需要利用前向传播的某些中间结果，需要进行保存，否则前向传播结束后这些对象即被释放\n",
    "\n",
    "Function的使用利用Function.apply(variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mZOZDRlWQ-oe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "3-Autograd.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
