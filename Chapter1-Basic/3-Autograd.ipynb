{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3-Autograd.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"oIfkscXCrgyv","colab_type":"text"},"source":["# Autograd    \n","`torch.autograd` 就是为方便用户使用，而专门开发的一套自动求导引擎，它能够根据输入和前向传播过程自动构建计算图，并执行反向传播。\n","\n","计算图(Computation Graph)是现代深度学习框架如PyTorch和TensorFlow等的核心，其为高效自动求导算法——反向传播(Back Propogation)提供了理论支持，了解计算图在实际写程序过程中会有极大的帮助。"]},{"cell_type":"markdown","metadata":{"id":"-3eEmVIcLJrs","colab_type":"text"},"source":["## 3.1 Autograd的前世今生       \n","深度学习的算法本质上是通过反向传播求导数，而PyTorch的**`autograd`**模块则实现了此功能。在Tensor上的所有操作，autograd都能为它们自动提供微分，避免了手动计算导数的复杂过程。\n"," \n","~~`autograd.Variable`是Autograd中的核心类，它简单封装了Tensor，并支持几乎所有Tensor有的操作。Tensor在被封装为Variable之后，可以调用它的`.backward`实现反向传播，自动计算所有梯度~~\n","\n","\n","~~Variable主要包含三个属性~~      \n","~~- `data`：保存Variable所包含的Tensor~~       \n","~~- `grad`：保存`data`对应的梯度，`grad`也是个Variable，而不是Tensor，它和`data`的形状一样~~        \n","~~- `grad_fn`：指向一个`Function`对象，这个`Function`用来反向传播计算输入的梯度~~             \n","\n","---     \n","\n","\n","  *从0.4起, Variable 正式合并入Tensor, Variable 本来实现的自动微分功能，Tensor就能支持。读者还是可以使用Variable(tensor), 但是这个操作其实什么都没做。建议读者以后直接使用tensor*. \n","  \n","  要想使得Tensor使用autograd功能，只需要设置**`tensor.requries_grad=True`**. "]},{"cell_type":"markdown","metadata":{"id":"HSotMu90-NqB","colab_type":"text"},"source":["## 3.2  requires_grad\n","PyTorch在autograd模块中实现了计算图的相关功能，autograd中的核心数据结构是Variable。从v0.4版本起，Variable和Tensor合并。我们可以认为需要求导(requires_grad)的tensor即Variable. autograd记录对tensor的操作记录用来构建计算图。\n","\n","Variable提供了大部分tensor支持的函数，但其不支持部分`inplace`函数，因这些函数会修改tensor自身，而在反向传播中，variable需要缓存原来的tensor来计算反向传播梯度。如果想要计算各个Variable的梯度，只需调用根节点variable的`backward`方法，autograd会自动沿着计算图反向传播，计算每一个叶子节点的梯度。\n","\n","***`variable.backward(gradient=None, retain_graph=None, create_graph=None)`***        \n","主要有如下参数：\n","\n","- grad_variables：形状与variable一致，对于`y.backward()`，grad_variables相当于链式法则${dz \\over dx}={dz \\over dy} \\times {dy \\over dx}$中的$\\textbf {dz} \\over \\textbf {dy}$。grad_variables也可以是tensor或序列。\n","- retain_graph：反向传播需要缓存一些中间结果，反向传播之后，这些缓存就被清空，可通过指定这个参数不清空缓存，用来多次反向传播。\n","- create_graph：对反向传播过程再次构建计算图，可通过`backward of backward`实现求高阶导数"]},{"cell_type":"code","metadata":{"id":"5ODtAByD8TCi","colab_type":"code","colab":{}},"source":["import torch as t"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yPZ6_o6W9OSv","colab_type":"code","outputId":"9953868e-0cfd-43d7-c2b5-a1b553a374f5","executionInfo":{"status":"ok","timestamp":1557399783388,"user_tz":-480,"elapsed":2505,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["a = t.randn(2, 3, requires_grad=True)\n","\n","a = t.randn(2, 3).requires_grad_()\n","\n","a = t.rand(2, 3)\n","a.requires_grad=True\n","a"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.8913, 0.5391, 0.6613],\n","        [0.6033, 0.9297, 0.0086]], requires_grad=True)"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"2XxnMNeO9sCb","colab_type":"code","colab":{}},"source":["b = t.zeros(2, 3, requires_grad=True) "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5tBXrLjy_NOJ","colab_type":"code","outputId":"a72dc413-ab1e-4e54-fe62-02cf174079dc","executionInfo":{"status":"ok","timestamp":1557399783392,"user_tz":-480,"elapsed":2485,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["c = a.add(b)\n","c.grad_fn"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<AddBackward0 at 0x7f613807e518>"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"VTWfwi9j_law","colab_type":"code","outputId":"119ac533-a09b-4627-f743-415c7f749c63","executionInfo":{"status":"ok","timestamp":1557399783393,"user_tz":-480,"elapsed":2459,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["d = c.sum()\n","print(d)\n","d.backward\n","print(d)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["tensor(3.6332, grad_fn=<SumBackward0>)\n","tensor(3.6332, grad_fn=<SumBackward0>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"31DhpXn4_ssi","colab_type":"code","outputId":"3adbe5e1-d689-4c24-bf8a-5beea3243cf0","executionInfo":{"status":"ok","timestamp":1557399783396,"user_tz":-480,"elapsed":2446,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["a.is_leaf, b.is_leaf, c.is_leaf, d.is_leaf"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(True, True, False, False)"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"OV04_X3JAWGp","colab_type":"text"},"source":["利用下面的函数，比对一下自动求导与手动求导：      \n","$$\n","y = x^2\\bullet e^x\n","$$\n","它的导函数是：\n","$$\n","{dy \\over dx} = 2x\\bullet e^x + x^2 \\bullet e^x\n","$$"]},{"cell_type":"code","metadata":{"id":"zyu4MrhBAtyM","colab_type":"code","colab":{}},"source":["def f(x):\n","    y = x** 2 * t.exp(x)\n","    return y\n","\n","def gradf(x):\n","    dx = 2*x*t.exp(x) + x**2*t.exp(x)\n","    return dx"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1tNa1Gw3BM3w","colab_type":"code","outputId":"c25cad94-6ba6-4aed-923c-35399a7fb2ca","executionInfo":{"status":"ok","timestamp":1557399783400,"user_tz":-480,"elapsed":2432,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["x = t.randn(3, 4, requires_grad=True)\n","y = f(x)\n","y"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[2.0799e-02, 2.6712e-01, 2.8743e+01, 1.2769e-03],\n","        [8.3794e+00, 4.0024e-02, 2.3730e-01, 2.4196e-02],\n","        [2.9234e+00, 2.7208e-04, 3.1842e+01, 3.0278e+00]],\n","       grad_fn=<MulBackward0>)"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"N2YmZ_CtBtLT","colab_type":"code","outputId":"9fe1364b-a948-4600-8fd6-273fce8e1497","executionInfo":{"status":"ok","timestamp":1557399783401,"user_tz":-480,"elapsed":2415,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["y.backward(t.ones(y.size())) # gradient形状与y一致\n","x.grad"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 3.2935e-01, -4.4218e-01,  5.7688e+01, -6.8901e-02],\n","        [ 2.0166e+01,  4.7840e-01,  1.4267e+00,  3.5864e-01],\n","        [ 8.6308e+00,  3.3533e-02,  6.3099e+01,  8.8707e+00]])"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"xhNcjr-fB9uU","colab_type":"code","outputId":"b3f000b5-465c-4c10-8627-9e391915ac84","executionInfo":{"status":"ok","timestamp":1557399783402,"user_tz":-480,"elapsed":2398,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["# 手动计算  \n","gradf(x)"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 3.2935e-01, -4.4218e-01,  5.7688e+01, -6.8901e-02],\n","        [ 2.0166e+01,  4.7840e-01,  1.4267e+00,  3.5864e-01],\n","        [ 8.6308e+00,  3.3533e-02,  6.3099e+01,  8.8707e+00]],\n","       grad_fn=<AddBackward0>)"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"GJXzbI7DCX3U","colab_type":"text"},"source":["## 3.3 计算图     \n","PyTorch中`autograd`的底层采用了计算图，计算图是一种特殊的有向无环图（DAG），用于记录算子与变量之间的关系。一般用矩形表示算子，椭圆形表示变量。       \n","表达式 $ \\textbf {z = wx + b}$可分解为 $\\textbf{y = wx}$和 $\\textbf{z = y + b}$，其计算图如图所示，图中`MUL`，`ADD`都是算子，$\\textbf{w}$，$\\textbf{x}$，$\\textbf{b}$即变量。\n","\n","![computation graph](img/com_graph.svg)"]},{"cell_type":"markdown","metadata":{"id":"2FVUGB9QFQAx","colab_type":"text"},"source":["如上有向无环图中，$\\textbf{X}$和$\\textbf{b}$是叶子节点（leaf node），这些节点通常由用户自己创建，不依赖于其他变量。$\\textbf{z}$称为根节点，是计算图的最终目标。利用链式法则很容易求得各个叶子节点的梯度。\n","$${\\partial z \\over \\partial b} = 1,\\space {\\partial z \\over \\partial y} = 1\\\\\n","{\\partial y \\over \\partial w }= x,{\\partial y \\over \\partial x}= w\\\\\n","{\\partial z \\over \\partial x}= {\\partial z \\over \\partial y} {\\partial y \\over \\partial x}=1 * w\\\\\n","{\\partial z \\over \\partial w}= {\\partial z \\over \\partial y} {\\partial y \\over \\partial w}=1 * x\\\\\n","$$\n","而有了计算图，上述链式求导即可利用计算图的反向传播自动完成，如图所示。\n","\n","![反向传播](img/com_graph_backward.svg)"]},{"cell_type":"markdown","metadata":{"id":"jN2J7LJ2FjOh","colab_type":"text"},"source":["在PyTorch实现中，autograd会随着用户的操作，记录生成当前variable的所有操作，并由此建立一个有向无环图。用户每进行一个操作，相应的计算图就会发生改变。更底层的实现中，图中记录了操作`Function`，每一个变量在图中的位置可通过其`grad_fn`属性在图中的位置推测得到。      \n","  \n","在反向传播过程中，autograd沿着这个图从当前变量（根节点$\\textbf{z}$）溯源，可以利用链式求导法则计算所有叶子节点的梯度。每一个前向传播操作的函数都有与之对应的反向传播函数用来计算输入的各个variable的梯度，这些函数的函数名通常以`Backward`结尾"]},{"cell_type":"code","metadata":{"id":"z6CiwPZTGrQQ","colab_type":"code","colab":{}},"source":["x = t.ones(1)\n","w = t.rand(1, requires_grad=True)\n","b = t.rand(1, requires_grad=True)\n","\n","y = w * x\n","z = y + b"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kAfw-bI8HNGc","colab_type":"code","outputId":"902f72a5-d309-4de9-e4d2-9e9e52ff0bc8","executionInfo":{"status":"ok","timestamp":1557399783406,"user_tz":-480,"elapsed":2389,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# y依赖于需要求导的w故为True\n","x.requires_grad, w.requires_grad, b.requires_grad, y.requires_grad, z.requires_grad"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(False, True, True, True, True)"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"FiVjN3f4HW5h","colab_type":"code","outputId":"0f27c8e2-a25b-4996-d367-c833126b2479","executionInfo":{"status":"ok","timestamp":1557399783407,"user_tz":-480,"elapsed":2365,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["x.is_leaf, w.is_leaf, b.is_leaf, y.is_leaf, z.is_leaf"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(True, True, True, False, False)"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"SoOCQ1qaIBIu","colab_type":"code","outputId":"1f70d578-1c3e-4820-e246-19dc08ffa33c","executionInfo":{"status":"ok","timestamp":1557399784012,"user_tz":-480,"elapsed":2960,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# grad_fn：查看variable的反向传播函数\n","z.grad_fn"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<AddBackward0 at 0x7f60ec54bef0>"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"754vumJzILMl","colab_type":"code","outputId":"4de12831-d8eb-441c-f062-89bdc3771c3d","executionInfo":{"status":"ok","timestamp":1557399784014,"user_tz":-480,"elapsed":2952,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# next_functions：保存grad_fn的输入，是一个tuple，其中的元素也是Function\n","# 第一个是y，它是乘法(mul)的输出，所以y.grad_fn是MulBackward\n","# 第二个是b，它是叶子节点，由用户创建，grad_fn为None，但是有\n","z.grad_fn.next_functions"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((<MulBackward0 at 0x7f60ec56d7b8>, 0),\n"," (<AccumulateGrad at 0x7f60ec56d240>, 0))"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"XX3Hqh2RIRND","colab_type":"code","outputId":"dbc7a259-d447-48d3-ed23-3c3ecf1c447c","executionInfo":{"status":"ok","timestamp":1557399784015,"user_tz":-480,"elapsed":2943,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["z.grad_fn.next_functions[0][0] == y.grad_fn"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"1dpwvXI2I_3-","colab_type":"code","outputId":"4e2155ba-9e8b-4917-a1bb-1b50009526fd","executionInfo":{"status":"ok","timestamp":1557399784016,"user_tz":-480,"elapsed":2934,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# w, b\n","y.grad_fn.next_functions"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((<AccumulateGrad at 0x7f60ec56d0b8>, 0), (None, 0))"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"cWERdScuJHYN","colab_type":"code","outputId":"d1c48488-e540-4a2e-c54b-9931ecf0be43","executionInfo":{"status":"ok","timestamp":1557399784017,"user_tz":-480,"elapsed":2925,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["w.grad_fn, x.grad_fn"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(None, None)"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"fhE0jHoaJT5E","colab_type":"code","outputId":"85ba3757-ef40-4885-9cdd-cc6054d1754c","executionInfo":{"status":"ok","timestamp":1557399784018,"user_tz":-480,"elapsed":2916,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["z.backward(retain_graph=True)\n","w.grad"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1.])"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"yQtH4HP7WMw2","colab_type":"code","outputId":"62995b21-276f-41e1-8048-1b14de22088c","executionInfo":{"status":"ok","timestamp":1557399784019,"user_tz":-480,"elapsed":2906,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["z.backward()\n","w.grad"],"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([2.])"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"py8CgD1rWPtP","colab_type":"text"},"source":["PyTorch使用的是动态图，它的计算图在每次前向传播时都是从头开始构建，所以它能够使用Python控制语句（如for、if等）根据需求创建计算图。      \n","   \n","这点在自然语言处理领域中很有用，它意味着你不需要事先构建所有可能用到的图的路径，图在运行时才构建。"]},{"cell_type":"markdown","metadata":{"id":"_ApgeqYzXCMF","colab_type":"text"},"source":["变量的`requires_grad`属性默认为False，如果某一个节点requires_grad被设置为True，那么所有依赖它的节点`requires_grad`都是True。这其实很好理解，对于$ \\textbf{x}\\to \\textbf{y} \\to \\textbf{z}$，x.requires_grad = True，当需要计算$\\partial z \\over \\partial x$时，根据链式法则，$\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} \\frac{\\partial y}{\\partial x}$，自然也需要求$ \\frac{\\partial z}{\\partial y}$，所以`y.requires_grad` 会被自动标为True. \n","\n","\n","\n","有些时候我们可能不希望autograd对tensor求导。认为求导需要缓存许多中间结构，增加额外的内存/显存开销，那么我们可以关闭自动求导。对于不需要反向传播的情景（如inference，即测试推理时），关闭自动求导可实现一定程度的速度提升，并节省约一半显存，因其不需要分配空间计算梯度"]},{"cell_type":"code","metadata":{"id":"yJN5PwqzXjSo","colab_type":"code","outputId":"447d9581-2abf-4f1a-d5ea-6555fd09fb55","executionInfo":{"status":"ok","timestamp":1557399784020,"user_tz":-480,"elapsed":2898,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["w = t.rand(1, requires_grad=True)\n","x = t.ones(1, requires_grad=True)\n","y = w * x\n","w, x, y"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([0.4142], requires_grad=True),\n"," tensor([1.], requires_grad=True),\n"," tensor([0.4142], grad_fn=<MulBackward0>))"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"OQ29gtkNa_e_","colab_type":"text"},"source":["### 设置不自动求导"]},{"cell_type":"code","metadata":{"id":"b5_8n80GYd2W","colab_type":"code","outputId":"dbbf4e2a-6178-4c8a-f8f2-bbd23ce3921a","executionInfo":{"status":"ok","timestamp":1557399784021,"user_tz":-480,"elapsed":2884,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["with t.no_grad():\n","    w = t.rand(1, requires_grad=True)\n","    x = t.ones(1)\n","    y = w * x\n","# y依赖于w和x，但x和y\n","w, x, y    "],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([0.1583], requires_grad=True), tensor([1.]), tensor([0.1583]))"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"9NorZ-yLZk-U","colab_type":"code","outputId":"b920a985-7472-46e3-cd21-22e69803bda9","executionInfo":{"status":"ok","timestamp":1557399784023,"user_tz":-480,"elapsed":2869,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["t.set_grad_enabled(False) \n","w = t.rand(1, requires_grad=True)\n","x = t.ones(1)\n","y = w * x\n","# y依赖于w和x，但x和y\n","w, x, y"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([0.0417], requires_grad=True), tensor([1.]), tensor([0.0417]))"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"JuLYHT3YZ04z","colab_type":"code","outputId":"8abda27e-5c5c-4a59-8ca4-108d3d863977","executionInfo":{"status":"ok","timestamp":1557399784025,"user_tz":-480,"elapsed":2857,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["t.set_grad_enabled(True)"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch.autograd.grad_mode.set_grad_enabled at 0x7f60ec56d860>"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"hY4eqzwLbrKp","colab_type":"text"},"source":["### 修改Tensor数值    \n","如果我们希望对tensor，但是又不希望被记录, 可以使用 **`tensor.data` 或 `tensor.detach()`**"]},{"cell_type":"code","metadata":{"id":"ops7dKsGj0Lw","colab_type":"code","outputId":"3486ee31-32e5-4f3e-a769-a3af7b3355a4","executionInfo":{"status":"ok","timestamp":1557399784026,"user_tz":-480,"elapsed":2842,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["a = t.ones(3,4,requires_grad=True)\n","b = t.ones(3,4,requires_grad=True)\n","c = a * b  \n","a.data"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 1., 1., 1.],\n","        [1., 1., 1., 1.],\n","        [1., 1., 1., 1.]])"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"vdQEdH54kLwv","colab_type":"code","outputId":"34d20cb1-ca95-4bed-d524-ad02d7c28964","executionInfo":{"status":"ok","timestamp":1557399784027,"user_tz":-480,"elapsed":2827,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["a.data.requires_grad"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"zOC1q4fQkZuu","colab_type":"code","outputId":"3193fe56-385f-44f2-e634-006977af49e3","executionInfo":{"status":"ok","timestamp":1557399784036,"user_tz":-480,"elapsed":2826,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["d = a.data.sigmoid_()\n","d.requires_grad"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"P5QsfMCAkhY0","colab_type":"code","outputId":"65f99b71-a38f-4d29-e410-b5b86e3340e7","executionInfo":{"status":"ok","timestamp":1557399784038,"user_tz":-480,"elapsed":2772,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["a"],"execution_count":28,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.7311, 0.7311, 0.7311, 0.7311],\n","        [0.7311, 0.7311, 0.7311, 0.7311],\n","        [0.7311, 0.7311, 0.7311, 0.7311]], requires_grad=True)"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"dDuK4mBRklni","colab_type":"code","outputId":"56397d56-d18c-40ba-99bf-b6d540db100e","executionInfo":{"status":"ok","timestamp":1557399784041,"user_tz":-480,"elapsed":2762,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["# tensor.datch()\n","# 近似于 tensor=a.data, 但是如果tensor被修改，backward可能会报错\n","tensor = a.detach()\n","tensor, tensor.requires_grad"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[0.7311, 0.7311, 0.7311, 0.7311],\n","         [0.7311, 0.7311, 0.7311, 0.7311],\n","         [0.7311, 0.7311, 0.7311, 0.7311]]), False)"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"gnyf95Kjlsec","colab_type":"code","outputId":"2e915db2-98f5-4cc4-91f3-f8b464c30b33","executionInfo":{"status":"ok","timestamp":1557399784042,"user_tz":-480,"elapsed":2742,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# 统计tensor的一些指标，并不希望被记录\n","mean = tensor.mean()\n","std = tensor.std()\n","mean, std"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor(0.7311), tensor(0.))"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"CATv1Iexmih1","colab_type":"text"},"source":["### 查看变量梯度    \n","在反向传播过程中非叶子节点的导数计算完之后即被清空。若想查看这些变量的梯度，有两种方法：\n","- 使用autograd.grad函数\n","- 使用hook\n","\n","`autograd.grad`和`hook`方法都是很强大的工具。     \n","推荐使用`hook`方法，但是在实际使用中应尽量避免修改grad的值。"]},{"cell_type":"code","metadata":{"id":"S500yFysxujz","colab_type":"code","outputId":"d8ea2bca-0458-4347-d61d-0bcea517195f","executionInfo":{"status":"ok","timestamp":1557399784044,"user_tz":-480,"elapsed":2722,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["w = t.rand(1, requires_grad=True)\n","x = t.ones(1, requires_grad=True)\n","y = w * x\n","z = y.sum()\n","w, x, y, z"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([0.5200], requires_grad=True),\n"," tensor([1.], requires_grad=True),\n"," tensor([0.5200], grad_fn=<MulBackward0>),\n"," tensor(0.5200, grad_fn=<SumBackward0>))"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"j0GBgHKpx430","colab_type":"code","outputId":"9b876318-b893-4e0c-8534-bdf8d7df5cbf","executionInfo":{"status":"ok","timestamp":1557399784046,"user_tz":-480,"elapsed":2707,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# 非叶子节点grad计算完之后自动清空，y.grad是None\n","z.backward()\n","w.grad, x.grad, y.grad"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([1.]), tensor([0.5200]), None)"]},"metadata":{"tags":[]},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"bP67mmDPyDQL","colab_type":"code","outputId":"c09d1dc3-18ab-46d2-8867-c816cf9f907e","executionInfo":{"status":"ok","timestamp":1557399784050,"user_tz":-480,"elapsed":2686,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["# 方法一：\n","w = t.rand(3, requires_grad=True)\n","x = t.ones(3, requires_grad=True)\n","y = w * x\n","z = y.sum()\n","w, x, y, z"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([0.5702, 0.0767, 0.8931], requires_grad=True),\n"," tensor([1., 1., 1.], requires_grad=True),\n"," tensor([0.5702, 0.0767, 0.8931], grad_fn=<MulBackward0>),\n"," tensor(1.5401, grad_fn=<SumBackward0>))"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"kBo8OOwvzoun","colab_type":"code","outputId":"bafded55-26c8-444e-a044-5b86f57dd066","executionInfo":{"status":"ok","timestamp":1557399784053,"user_tz":-480,"elapsed":2670,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["t.autograd.grad(z, y)"],"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([1., 1., 1.]),)"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"EdKFPvZyz0rQ","colab_type":"code","outputId":"621d9588-19e8-4196-8c0f-98e7e81a9f60","executionInfo":{"status":"ok","timestamp":1557399784055,"user_tz":-480,"elapsed":2662,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# 方法二：使用hook\n","# hook是一个函数，输入是梯度，不应有返回值\n","def variable_hook(grad):\n","    print('y的梯度：', grad)\n","    \n","w = t.rand(3, requires_grad=True)\n","x = t.ones(3, requires_grad=True)\n","y = w * x\n","z = y.sum()\n","\n","hook = y.register_hook(variable_hook)\n","z.backward()\n","\n","# 若不重复使用hook，则移除\n","hook.remove()"],"execution_count":35,"outputs":[{"output_type":"stream","text":["y的梯度： tensor([1., 1., 1.])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"13Yh0dzN0y-K","colab_type":"text"},"source":["关于 variable中grad属性和backward函数`grad_variables`参数的含义：\n","\n","- variable $\\textbf{x}$的梯度是目标函数${f(x)} $对$\\textbf{x}$的梯度，$\\frac{df(x)}{dx} = (\\frac {df(x)}{dx_0},\\frac {df(x)}{dx_1},...,\\frac {df(x)}{dx_N})$，形状和$\\textbf{x}$一致。\n","- 对于y.backward(grad_variables)中的grad_variables相当于链式求导法则中的$\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} \\frac{\\partial y}{\\partial x}$中的$\\frac{\\partial z}{\\partial y}$。z是目标函数，一般是一个标量，故而$\\frac{\\partial z}{\\partial y}$的形状与variable $\\textbf{y}$的形状一致。`z.backward()`在一定程度上等价于y.backward(grad_y)。`z.backward()`省略了grad_variables参数，是因为$z$是一个标量，而$\\frac{\\partial z}{\\partial z} = 1$"]},{"cell_type":"code","metadata":{"id":"fy16ZsJd10Du","colab_type":"code","outputId":"6098e35f-b273-4c67-bc3c-879b0c7141c6","executionInfo":{"status":"ok","timestamp":1557399784057,"user_tz":-480,"elapsed":2655,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":71}},"source":["x = t.Tensor([0,1,2]).requires_grad_()\n","y = x**2 + 2*x\n","z = y.sum()\n","print(x, y, z)\n","\n","z.backward()\n","x.grad"],"execution_count":36,"outputs":[{"output_type":"stream","text":["tensor([0., 1., 2.], requires_grad=True) tensor([0., 3., 8.], grad_fn=<AddBackward0>) tensor(11., grad_fn=<SumBackward0>)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["tensor([2., 4., 6.])"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"code","metadata":{"id":"VJ7H4G-b5XHW","colab_type":"code","outputId":"bf6a0d15-97bc-48e9-9bd3-822bef7c3f3a","executionInfo":{"status":"ok","timestamp":1557399784059,"user_tz":-480,"elapsed":2648,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["x = t.Tensor([0,1,2]).requires_grad_()\n","y = x**2 + 2*x\n","z = y.sum()\n","\n","y_gredient = t.Tensor([1, 1, 1]) # dz/dy\n","y.backward(y_gredient)\n","x.grad"],"execution_count":37,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([2., 4., 6.])"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"markdown","metadata":{"id":"ILZoPhf5MDdi","colab_type":"text"},"source":["另外值得注意的是，只有对variable的操作才能使用autograd，如果对variable的data直接进行操作，将无法使用反向传播。除了对参数初始化，一般我们不会修改variable.data的值"]},{"cell_type":"markdown","metadata":{"id":"AqtRW2RLMTcm","colab_type":"text"},"source":["### 计算图的特点：\n","- autograd根据用户对variable的操作构建其计算图。对变量的操作抽象为`Function`。\n","- 对于那些不是任何函数(Function)的输出，由用户创建的节点称为叶子节点，叶子节点的`grad_fn`为None。叶子节点中需要求导的variable，具有`AccumulateGrad`标识，因其梯度是累加的。\n","- variable默认是不需要求导的，即`requires_grad`属性默认为False，如果某一个节点requires_grad被设置为True，那么所有依赖它的节点`requires_grad`都为True。\n","- variable的`volatile`属性默认为False，如果某一个variable的`volatile`属性被设为True，那么所有依赖它的节点`volatile`属性都为True。volatile属性为True的节点不会求导，volatile的优先级比`requires_grad`高。\n","- 多次反向传播时，梯度是累加的。反向传播的中间缓存会被清空，为进行多次反向传播需指定`retain_graph`=True来保存这些缓存。\n","- 非叶子节点的梯度计算完之后即被清空，可以使用`autograd.grad`或`hook`技术获取非叶子节点的值。\n","- variable的grad与data形状一致，应避免直接修改variable.data，因为对data的直接操作无法利用autograd进行反向传播\n","- 反向传播函数`backward`的参数`grad_variables`可以看成链式求导的中间结果，如果是标量，可以省略，默认为1\n","- PyTorch采用动态图设计，可以很方便地查看中间层的输出，动态的设计计算图结构"]},{"cell_type":"markdown","metadata":{"id":"43Qf2VcrJ2QL","colab_type":"text"},"source":["## 3.4 Autograd拓展         \n","\n","目前绝大多数函数都可以使用`autograd`实现反向求导，但如果需要自己写一个复杂的函数，不支持自动反向求导怎么办? 写一个`Function`，实现它的前向传播和反向传播代码，`Function`对应于计算图中的矩形， 它接收参数，计算并返回结果。     \n","\n","\n","---\n","\n","\n","- 自定义的Function需要继承autograd.Function，没有构造函数`__init__`，forward和backward函数都是静态方法\n","- backward函数的输出和forward函数的输入一一对应，backward函数的输入和forward函数的输出一一对应\n","- backward函数的grad_output参数即t.autograd.backward中的`grad_variables`\n","- 如果某一个输入不需要求导，直接返回None，如forward中的输入参数x_requires_grad显然无法对它求导，直接返回None即可\n","- 反向传播可能需要利用前向传播的某些中间结果，需要进行保存，否则前向传播结束后这些对象即被释放\n","\n","Function的使用利用Function.apply(variable)"]},{"cell_type":"code","metadata":{"id":"mZOZDRlWQ-oe","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}