{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PRoRd2jwOz58"
   },
   "source": [
    "## 线性回归     \n",
    "\n",
    "线性回归是机器学习入门知识，应用十分广泛。线性回归利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的，其表达形式为$y = wx+b+e$，$e$为误差服从均值为0的正态分布。        \n",
    "\n",
    "首先让我们来确认线性回归的损失函数：\n",
    "$$\n",
    "loss = \\sum_i^N ({(wx_i+b) - y_i})^2\n",
    "$$\n",
    "然后利用 梯度下降法更新参数$\\textbf{w}$和$\\textbf{b}$来最小化损失函数，最终学得$\\textbf{w}$和$\\textbf{b}$的数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AeMIsLriPStb"
   },
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OOYdrnAdSMUD"
   },
   "outputs": [],
   "source": [
    "# 计算总loss\n",
    "def compute_totalError(w, b, points):\n",
    "    totalError = 0\n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        totalError += (y - (w * x + b)) ** 2\n",
    "    return totalError / float(len(points))\n",
    "\n",
    "\n",
    "# 计算每一步更新的梯度\n",
    "def step_gradient(cur_w, cur_b, points, lr):\n",
    "    cur_w = 0\n",
    "    cur_b = 0\n",
    "    N = float(len(points))\n",
    "    \n",
    "    for i in range(0, len(points)):\n",
    "        x = points[i, 0]\n",
    "        y = points[i, 1]\n",
    "        w_grad = (2/N) * ((cur_w*x + cur_b) - y) * x\n",
    "        b_grad = (2/N) * ((cur_w*x + cur_b) - y)\n",
    "    new_w = w - lr * w_grad\n",
    "    new_b = b - lr * b_grad\n",
    "    \n",
    "    return [new_w, new_b]\n",
    "\n",
    "def gd_runner(points, start_w, start_b, lr, num_iters):\n",
    "    w = start_w\n",
    "    b = start_b\n",
    "    \n",
    "    for i in range(int(num_iters)):\n",
    "        w, b = step_gradient(w, b, np.array(points), lr)\n",
    "    # 返回最后一组 w,b    \n",
    "    return [w, b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent at w = 0, b = 0, error = 5565.107834483211\n",
      "Running...\n",
      "<class 'list'>\n",
      "After 1000 iterations, w = tensor([[0.6615]]), b = tensor([[0.0011]]), error=tensor([[1779.1470]])\n"
     ]
    }
   ],
   "source": [
    "def run():\n",
    "    points = np.genfromtxt('data.csv', delimiter=',')\n",
    "    lr = 0.001\n",
    "    initial_w = 0\n",
    "    initial_b = 0\n",
    "    num_iters = 1000\n",
    "    \n",
    "    print('Starting gradient descent at w = {0}, b = {1}, error = {2}'\n",
    "          .format(initial_w, initial_b, compute_totalError(initial_w, initial_b, points)))\n",
    "    \n",
    "    print('Running...')\n",
    "    \n",
    "    [w, b] = gd_runner(points, initial_w, initial_b, lr, num_iters)\n",
    "    print('After {0} iterations, w = {1}, b = {2}, error={3}'\n",
    "         .format(num_iters, w, b, compute_totalError(w, b, points)))\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "4-LinearRegression.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
