{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"1-torch_nn.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"IzuRr6idRsEb","colab_type":"text"},"source":["# torch.nn           \n","torch.nn是专门为深度学习而设计的模块。torch.nn的核心数据结构是`Module`，它是一个抽象概念，既可以表示神经网络中的某个层（layer），也可以表示一个包含很多层的神经网络。   \n","在实际使用中，最常见的做法是继承`nn.Module`，撰写自己的网络/层。"]},{"cell_type":"markdown","metadata":{"id":"LUCXPWCXSSgy","colab_type":"text"},"source":["## 1.1 自定义层              \n","如何用nn.Module实现自己的全连接层。全连接层，又名仿射层，输出$\\textbf{y}$和输入$\\textbf{x}$满足$\\textbf{y=Wx+b}$，$\\textbf{W}$和$\\textbf{b}$是可学习的参数"]},{"cell_type":"code","metadata":{"id":"pm9HAYlASsIG","colab_type":"code","colab":{}},"source":["import torch as t\n","from torch import nn"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m1NTA17vS5nj","colab_type":"code","colab":{}},"source":["class Linear(nn.Module):\n","    def __init__(self, in_f, out_f):\n","        # 等价于 nn.Module.__init(self)\n","        super(Linear, self).__init__()\n","        self.w = nn.Parameter(t.randn(in_f, out_f))\n","        self.b = nn.Parameter(t.randn(out_f))\n","        \n","    def forward(self, x):\n","        x = x.mm(self.w)\n","        return x + self.b.expand_as(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GyWG9voiUfcU","colab_type":"code","outputId":"5ff94323-c48d-4212-8ffc-f2f90c5c9999","executionInfo":{"status":"ok","timestamp":1557401213079,"user_tz":-480,"elapsed":4756,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["layer = Linear(4, 3)\n","input = t.randn(2, 4)\n","output = layer(input)\n","output"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[-6.1545, -2.6879, -4.4222],\n","        [ 0.8841, -2.5078, -2.4721]], grad_fn=<AddBackward0>)"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"5Pev8-_SVRad","colab_type":"code","outputId":"548e65f0-963f-4896-ef01-c80bd7fe3022","executionInfo":{"status":"ok","timestamp":1557401213081,"user_tz":-480,"elapsed":4740,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["for name, parameter in layer.named_parameters():\n","    print(name, parameter)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["w Parameter containing:\n","tensor([[-1.3030,  0.5937,  0.0578],\n","        [ 0.0861, -0.2141, -1.4550],\n","        [ 1.5023,  0.2462, -0.1321],\n","        [-0.6188, -0.6013, -1.1792]], requires_grad=True)\n","b Parameter containing:\n","tensor([-1.7979, -1.4262,  0.0190], requires_grad=True)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"sk2x8c08Vq6K","colab_type":"text"},"source":["### 在自定义层时，需注意以下几点：\n","- 自定义层`Linear`必须继承`nn.Module`，并且在其构造函数中需调用`nn.Module`的构造函数，即`super(Linear, self).__init__()` 或`nn.Module.__init__(self)`，推荐使用第一种用法，尽管第二种写法更直观。\n","- 在构造函数`__init__`中必须自己定义可学习的参数，并封装成`Parameter`，如在本例中我们把`w`和`b`封装成`parameter`。`parameter`是一种特殊的`Tensor`，但其默认需要求导（requires_grad = True），感兴趣的读者可以通过`nn.Parameter??`，查看`Parameter`类的源代码。\n","- `forward`函数实现前向传播过程，其输入可以是一个或多个tensor。\n","- 无需写反向传播函数，nn.Module能够利用autograd自动实现反向传播，这点比Function简单许多。\n","- 使用时，直观上可将layer看成数学概念中的函数，调用layer(input)即可得到input对应的结果。它等价于`layers.__call__(input)`，在`__call__`函数中，主要调用的是 `layer.forward(x)`，另外还对钩子做了一些处理。所以在实际使用中应尽量使用`layer(x)`而不是使用`layer.forward(x)`\n","- `Module`中的可学习参数可以通过`named_parameters()`或者`parameters()`返回迭代器，前者会给每个parameter都附上名字，使其更具有辨识度。\n","\n","可见利用Module实现的全连接层，比利用`Function`实现的更为简单，因其不再需要写反向传播函数"]},{"cell_type":"markdown","metadata":{"id":"QMya-TENXXRh","colab_type":"text"},"source":["## 1.2 多层感知机      \n","Module能够自动检测到自己的`Parameter`，并将其作为学习参数。除了`parameter`之外，Module还包含子`Module`，主Module能够递归查找子`Module`中的`parameter`。    \n","下面再来看看稍微复杂一点的网络，多层感知机。"]},{"cell_type":"code","metadata":{"id":"HpjiV_FJYm8r","colab_type":"code","colab":{}},"source":["class Perceptron(nn.Module):\n","    def __init__(self, in_f, hidden_f, out_f):\n","        nn.Module.__init__(self)\n","        # 利用前面定义的全连接层\n","        self.layer1 = Linear(in_f, hidden_f)\n","        self.layer2 = Linear(hidden_f, out_f)\n","        \n","    def forward(self, x):\n","        x = self.layer1(x)\n","        x = t.sigmoid(x)\n","        return self.layer2(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SSCEp3_3Y2A5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":85},"outputId":"2ab5027a-94c1-44de-c010-9e8003836778","executionInfo":{"status":"ok","timestamp":1557402216865,"user_tz":-480,"elapsed":1618,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}}},"source":["perceptron = Perceptron(3, 4, 1)\n","input = t.randn(4, 3)\n","out = perceptron(input)\n","out"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 0.6573],\n","        [ 1.4813],\n","        [-0.5200],\n","        [ 2.0085]], grad_fn=<AddBackward0>)"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"djS_nWkoaF9H","colab_type":"code","outputId":"f55966ea-ec0b-46c9-898e-32659a459787","executionInfo":{"status":"ok","timestamp":1557402222147,"user_tz":-480,"elapsed":1559,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":238}},"source":["for name, param in perceptron.named_parameters():\n","    print(name, param, param.size())"],"execution_count":13,"outputs":[{"output_type":"stream","text":["layer1.w Parameter containing:\n","tensor([[ 0.6701,  1.1535, -0.0547, -0.8057],\n","        [-1.6345,  0.2124,  1.8351, -1.8436],\n","        [-0.1596,  0.0630, -0.3726,  0.7524]], requires_grad=True) torch.Size([3, 4])\n","layer1.b Parameter containing:\n","tensor([-1.0419, -0.2923,  1.3164,  0.7752], requires_grad=True) torch.Size([4])\n","layer2.w Parameter containing:\n","tensor([[-0.9762],\n","        [-1.5965],\n","        [ 1.0703],\n","        [ 0.4736]], requires_grad=True) torch.Size([4, 1])\n","layer2.b Parameter containing:\n","tensor([0.8269], requires_grad=True) torch.Size([1])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UEL0sdg-aYzS","colab_type":"text"},"source":["可见，即使是稍复杂的多层感知机，其实现依旧很简单。 构造函数`__init__`中，可利用前面自定义的Linear层(module)，作为当前module对象的一个子module，它的可学习参数，也会成为当前module的可学习参数。\n","\n","### module中parameter的命名规范：\n","- 对于类似`self.param_name = nn.Parameter(t.randn(3, 4))`，命名为`param_name`\n","- 对于子Module中的parameter，会其名字之前加上当前Module的名字。如对于`self.sub_module = SubModel()`，SubModel中有个parameter的名字叫做param_name，那么二者拼接而成的parameter name 就是`sub_module.param_name`。"]},{"cell_type":"markdown","metadata":{"id":"gXmmKBRIbuMu","colab_type":"text"},"source":["### PS：查看关于layer文档  \n","为方便用户使用，PyTorch实现了神经网络中绝大多数的layer，这些layer都继承于nn.Module，封装了可学习参数`parameter`，并实现了forward函数，且很多都专门针对GPU运算进行了CuDNN优化，其速度和性能都十分优异。       \n","\n","对nn.Module中的所有层的具体内容，读者可参照官方文档或在IPython/Jupyter中使用nn.layer?来查看。阅读文档时应主要关注以下几点：\n","\n","- 构造函数的参数，如nn.Linear(in_features, out_features, bias)，需关注这三个参数的作用。\n","- 属性、可学习参数和子module。如nn.Linear中有`weight`和`bias`两个可学习参数，不包含子module。\n","- 输入输出的形状，如nn.linear的输入形状是(N, input_features)，输出为(N，output_features)，N是batch_size。\n","\n","这些自定义layer对输入形状都有假设：输入的不是单个数据，而是一个batch。输入只有一个数据，则必须调用`tensor.unsqueeze(0)` 或 `tensor[None]`将数据伪装成batch_size=1的batch"]},{"cell_type":"markdown","metadata":{"id":"VJwQYtfNb7B-","colab_type":"text"},"source":["## 1.3 nn.functional       \n","也是nn中常用的模块，nn中的大多数layer，在`functional`中都有一个与之相对应的函数。   \n","\n","`nn.functional`中的函数和`nn.Module`的主要区别：   \n","- 用nn.Module实现的layers是一个特殊的类，都是由`class layer(nn.Module)`定义，会自动提取可学习的参数   \n","- `nn.functional`中的函数更像是纯函数，由`def function(input)`定义    \n","\n","下面举例说明functional的使用，并指出二者的不同之处"]},{"cell_type":"code","metadata":{"id":"1GfwX4gWoECP","colab_type":"code","outputId":"a09872a0-bd89-4e37-f7bc-39a85193858f","executionInfo":{"status":"ok","timestamp":1557401213086,"user_tz":-480,"elapsed":4706,"user":{"displayName":"wes V","photoUrl":"https://lh5.googleusercontent.com/-hBncdTXpTfU/AAAAAAAAAAI/AAAAAAAAABI/U2Of_yQGm9M/s64/photo.jpg","userId":"14257616964780137484"}},"colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["input = t.randn(2, 3)\n","model = nn.Linear(3, 4)\n","output1 = model(input)\n","output2 = nn.functional.linear(input, model.weight, model.bias)\n","output1, output2"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(tensor([[-0.4439, -0.2176,  0.3737, -0.9774],\n","         [-0.5911, -0.1468,  0.5085, -0.7642]], grad_fn=<AddmmBackward>),\n"," tensor([[-0.4439, -0.2176,  0.3737, -0.9774],\n","         [-0.5911, -0.1468,  0.5085, -0.7642]], grad_fn=<AddmmBackward>))"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"Rv4yy7vQpXh1","colab_type":"text"},"source":["### 如何选择`nn.Module`和`nn.functional`     \n","模型有可学习的参数，最好用nn.Module，否则既可以使用nn.functional也可以使用nn.Module，二者在性能上没有太大差异，具体的使用取决于个人的喜好。      \n","\n","如激活函数（ReLU、sigmoid、tanh），池化（MaxPool）等层由于没有可学习参数，则可以使用对应的functional函数代替，而对于卷积、全连接等具有可学习参数的网络建议使用nn.Module    \n","\n","**Dropout**    \n","虽然dropout操作也没有可学习操作，但建议还是使用`nn.Dropout`而不是`nn.functional.dropout`，因为dropout在训练和测试两个阶段的行为有所差别，使用`nn.Module`对象能够通过`model.eval`操作加以区分"]},{"cell_type":"code","metadata":{"id":"JGozWbWIqy8h","colab_type":"code","colab":{}},"source":["from torch.nn import functional as f\n","\n","class net(nn.Module):\n","    def __init__(self):\n","        super(net, self).__init__()\n","        self.conv1 = nn.Conv2d(3, 6, 5)\n","        self.conv2 = nn.Conv2d(6, 16, 5)"],"execution_count":0,"outputs":[]}]}