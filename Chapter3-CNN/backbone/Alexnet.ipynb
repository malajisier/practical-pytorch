{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Alexnet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkACQ8TKE-ER",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch as t\n",
        "import numpy as np\n",
        "from torch import nn\n",
        "from torchvision.datasets import CIFAR10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZbLkY-GE_Hf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Alexnet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Alexnet, self).__init__()\n",
        "        \n",
        "        # nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0,\n",
        "        # dilation=1, groups=1, bias=True, padding_mode='zeros',)\n",
        "        self.conv1 = nn.Sequential(nn.Conv2d(3, 64, 5), nn.ReLU(True))\n",
        "        self.maxpool1 = nn.MaxPool2d(3, 2)\n",
        "        self.conv2 = nn.Sequential(nn.Conv2d(64, 64, 5, 1), nn.ReLU(True))\n",
        "        self.maxpool2 = nn.MaxPool2d(3, 2)\n",
        "        self.fc1 = nn.Sequential(nn.Linear(1024, 384), nn.ReLU(True))\n",
        "        self.fc2 = nn.Sequential(nn.Linear(384, 192), nn.ReLU(True))\n",
        "        self.fc3 = nn.Linear(192, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxpool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.maxpool2(x)\n",
        "        \n",
        "        # 拉平矩阵\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8UIgEW9FB_F",
        "colab_type": "code",
        "outputId": "72c404f2-b0aa-4578-ae18-8baeef8316cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "alexnet = Alexnet()\n",
        "alexnet"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Alexnet(\n",
              "  (conv1): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(5, 5), stride=(1, 1))\n",
              "    (1): ReLU(inplace)\n",
              "  )\n",
              "  (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Sequential(\n",
              "    (0): Conv2d(64, 64, kernel_size=(5, 5), stride=(1, 1))\n",
              "    (1): ReLU(inplace)\n",
              "  )\n",
              "  (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (fc1): Sequential(\n",
              "    (0): Linear(in_features=1024, out_features=384, bias=True)\n",
              "    (1): ReLU(inplace)\n",
              "  )\n",
              "  (fc2): Sequential(\n",
              "    (0): Linear(in_features=384, out_features=192, bias=True)\n",
              "    (1): ReLU(inplace)\n",
              "  )\n",
              "  (fc3): Linear(in_features=192, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "939hIvb8FEJ-",
        "colab_type": "code",
        "outputId": "8f21aeda-2c71-48c5-819f-7b8ade12157e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_input = t.zeros((1, 3, 32, 32), requires_grad=True)\n",
        "out = alexnet(test_input)\n",
        "out.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLC-FGAHFGQG",
        "colab_type": "code",
        "outputId": "aad308e2-44c1-4a1d-953d-fb54f7e4e377",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "def data_tf(x):\n",
        "    x = np.array(x, dtype='float32') / 255\n",
        "    x = (x - 0.5) / 0.5 # 标准化，这个技巧之后会讲到\n",
        "    x = x.transpose((2, 0, 1)) # 将 channel 放到第一维，只是 pytorch 要求的输入方式\n",
        "    x = t.from_numpy(x)\n",
        "    return x\n",
        "     \n",
        "train_set = CIFAR10('./data', train=True, transform=data_tf, download=True)\n",
        "train_data = t.utils.data.DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "test_set = CIFAR10('./data', train=False, transform=data_tf, download=True)\n",
        "test_data = t.utils.data.DataLoader(test_set, batch_size=128, shuffle=False)\n",
        "\n",
        "device = t.device('cuda')\n",
        "net = Alexnet().to(device)\n",
        "optimizer = t.optim.SGD(net.parameters(), lr=1e-1)\n",
        "criterion = nn.CrossEntropyLoss().to(device)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "170500096it [00:09, 17806953.19it/s]                               \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-pFXhnLLNoz",
        "colab_type": "code",
        "outputId": "52f39d1f-b4d7-4e91-cb9d-78b5adba49ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from train_util import train\n",
        "\n",
        "train(net, train_data, test_data, 20, optimizer, criterion)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [0]\n",
            "Train loss: 1331.659947, Train acc: 0.250\n",
            "Test loss: 127.298236, Test acc: 0.438\n",
            "Time 00:00:08\n",
            "-------------------------------------------------\n",
            "Epoch [1]\n",
            "Train loss: 983.936461, Train acc: 0.500\n",
            "Test loss: 108.496559, Test acc: 0.500\n",
            "Time 00:00:16\n",
            "-------------------------------------------------\n",
            "Epoch [2]\n",
            "Train loss: 816.956243, Train acc: 0.812\n",
            "Test loss: 84.424404, Test acc: 0.875\n",
            "Time 00:00:24\n",
            "-------------------------------------------------\n",
            "Epoch [3]\n",
            "Train loss: 694.969443, Train acc: 0.625\n",
            "Test loss: 86.515812, Test acc: 0.750\n",
            "Time 00:00:32\n",
            "-------------------------------------------------\n",
            "Epoch [4]\n",
            "Train loss: 600.708918, Train acc: 0.812\n",
            "Test loss: 81.601642, Test acc: 0.688\n",
            "Time 00:00:40\n",
            "-------------------------------------------------\n",
            "Epoch [5]\n",
            "Train loss: 522.032478, Train acc: 0.750\n",
            "Test loss: 86.126449, Test acc: 0.375\n",
            "Time 00:00:48\n",
            "-------------------------------------------------\n",
            "Epoch [6]\n",
            "Train loss: 457.999559, Train acc: 0.812\n",
            "Test loss: 72.813909, Test acc: 0.688\n",
            "Time 00:00:56\n",
            "-------------------------------------------------\n",
            "Epoch [7]\n",
            "Train loss: 395.471013, Train acc: 0.812\n",
            "Test loss: 93.218124, Test acc: 0.625\n",
            "Time 00:01:03\n",
            "-------------------------------------------------\n",
            "Epoch [8]\n",
            "Train loss: 347.225296, Train acc: 0.625\n",
            "Test loss: 89.901097, Test acc: 0.625\n",
            "Time 00:01:11\n",
            "-------------------------------------------------\n",
            "Epoch [9]\n",
            "Train loss: 288.115120, Train acc: 0.750\n",
            "Test loss: 79.164429, Test acc: 0.562\n",
            "Time 00:01:19\n",
            "-------------------------------------------------\n",
            "Epoch [10]\n",
            "Train loss: 246.646206, Train acc: 0.750\n",
            "Test loss: 93.096747, Test acc: 0.688\n",
            "Time 00:01:27\n",
            "-------------------------------------------------\n",
            "Epoch [11]\n",
            "Train loss: 217.876930, Train acc: 0.750\n",
            "Test loss: 136.869754, Test acc: 0.625\n",
            "Time 00:01:35\n",
            "-------------------------------------------------\n",
            "Epoch [12]\n",
            "Train loss: 187.703285, Train acc: 1.000\n",
            "Test loss: 94.868280, Test acc: 0.750\n",
            "Time 00:01:43\n",
            "-------------------------------------------------\n",
            "Epoch [13]\n",
            "Train loss: 154.024307, Train acc: 0.938\n",
            "Test loss: 97.127622, Test acc: 0.750\n",
            "Time 00:01:51\n",
            "-------------------------------------------------\n",
            "Epoch [14]\n",
            "Train loss: 146.299415, Train acc: 0.938\n",
            "Test loss: 95.281536, Test acc: 0.500\n",
            "Time 00:01:59\n",
            "-------------------------------------------------\n",
            "Epoch [15]\n",
            "Train loss: 124.965571, Train acc: 1.000\n",
            "Test loss: 99.193800, Test acc: 0.688\n",
            "Time 00:02:07\n",
            "-------------------------------------------------\n",
            "Epoch [16]\n",
            "Train loss: 114.060168, Train acc: 0.812\n",
            "Test loss: 195.400093, Test acc: 0.562\n",
            "Time 00:02:14\n",
            "-------------------------------------------------\n",
            "Epoch [17]\n",
            "Train loss: 114.123798, Train acc: 0.938\n",
            "Test loss: 117.627489, Test acc: 0.688\n",
            "Time 00:02:22\n",
            "-------------------------------------------------\n",
            "Epoch [18]\n",
            "Train loss: 92.534889, Train acc: 0.938\n",
            "Test loss: 112.393992, Test acc: 0.500\n",
            "Time 00:02:30\n",
            "-------------------------------------------------\n",
            "Epoch [19]\n",
            "Train loss: 80.519683, Train acc: 1.000\n",
            "Test loss: 120.023541, Test acc: 0.750\n",
            "Time 00:02:38\n",
            "-------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}